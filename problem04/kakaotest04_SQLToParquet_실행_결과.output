2020-08-26 20:28:37,927 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2020-08-26 20:28:37,962 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2020-08-26 20:28:37,962 INFO impl.MetricsSystemImpl: JobTracker metrics system started
2020-08-26 20:28:38,091 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-08-26 20:28:38,414 INFO mapreduce.JobSubmitter: number of splits:1
2020-08-26 20:28:38,490 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1532807023_0001
2020-08-26 20:28:38,490 INFO mapreduce.JobSubmitter: Executing with tokens: []
2020-08-26 20:28:38,553 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
2020-08-26 20:28:38,553 INFO mapreduce.Job: Running job: job_local1532807023_0001
2020-08-26 20:28:38,554 INFO mapred.LocalJobRunner: OutputCommitter set in config null
2020-08-26 20:28:38,556 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2020-08-26 20:28:38,556 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2020-08-26 20:28:38,557 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.parquet.hadoop.ParquetOutputCommitter
2020-08-26 20:28:38,599 INFO mapred.LocalJobRunner: Waiting for map tasks
2020-08-26 20:28:38,599 INFO mapred.LocalJobRunner: Starting task: attempt_local1532807023_0001_m_000000_0
2020-08-26 20:28:38,611 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2020-08-26 20:28:38,611 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2020-08-26 20:28:38,620 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2020-08-26 20:28:38,625 INFO mapred.MapTask: Processing split: org.apache.hadoop.mapreduce.lib.db.DBInputFormat$DBInputSplit@2dbee48a
2020-08-26 20:28:38,632 INFO codec.CodecConfig: Compression set to false
2020-08-26 20:28:38,633 INFO codec.CodecConfig: Compression: UNCOMPRESSED
2020-08-26 20:28:38,640 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
2020-08-26 20:28:38,640 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
2020-08-26 20:28:38,640 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2020-08-26 20:28:38,640 INFO hadoop.ParquetOutputFormat: Dictionary is on
2020-08-26 20:28:38,640 INFO hadoop.ParquetOutputFormat: Validation is off
2020-08-26 20:28:38,640 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2020-08-26 20:28:38,640 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes
2020-08-26 20:28:38,640 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
2020-08-26 20:28:38,640 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2020-08-26 20:28:38,640 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2020-08-26 20:28:38,779 INFO mapred.LocalJobRunner:
2020-08-26 20:28:38,780 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,046
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [file:/tmp/hadoop-unjar7899794536715163230/shaded/parquet/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/woosung/kakao-coding-test/problem04/kakaotest04/out/artifacts/kakaotest04_jar/kakaotest04.jar!/shaded/parquet/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [shaded.parquet.org.slf4j.helpers.NOPLoggerFactory]
2020-08-26 20:28:38,939 INFO hadoop.ColumnChunkPageWriteStore: written 11,740B for [logTimestamp] BINARY: 649 values, 11,682B raw, 11,682B comp, 1 pages, encodings: [PLAIN]
2020-08-26 20:28:38,940 INFO hadoop.ColumnChunkPageWriteStore: written 5,879B for [logID] BINARY: 649 values, 5,841B raw, 5,841B comp, 1 pages, encodings: [PLAIN]
2020-08-26 20:28:38,954 INFO hadoop.ColumnChunkPageWriteStore: written 174B for [userNo] BINARY: 649 values, 140B raw, 140B comp, 1 pages, encodings: [PLAIN_DICTIONARY], dic { 5 entries, 35B raw, 5B comp}
2020-08-26 20:28:38,954 INFO hadoop.ColumnChunkPageWriteStore: written 374B for [menuName] BINARY: 649 values, 331B raw, 331B comp, 1 pages, encodings: [PLAIN_DICTIONARY], dic { 12 entries, 176B raw, 12B comp}
2020-08-26 20:28:39,147 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-08-26 20:28:39,556 INFO mapreduce.Job: Job job_local1532807023_0001 running in uber mode : false
2020-08-26 20:28:39,557 INFO mapreduce.Job:  map 0% reduce 0%
2020-08-26 20:28:39,659 INFO mapred.Task: Task:attempt_local1532807023_0001_m_000000_0 is done. And is in the process of committing
2020-08-26 20:28:39,662 INFO mapred.LocalJobRunner:
2020-08-26 20:28:39,662 INFO mapred.Task: Task attempt_local1532807023_0001_m_000000_0 is allowed to commit now
2020-08-26 20:28:39,681 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1532807023_0001_m_000000_0' to hdfs://localhost:9000/parquet_output
2020-08-26 20:28:39,682 INFO mapred.LocalJobRunner: map
2020-08-26 20:28:39,682 INFO mapred.Task: Task 'attempt_local1532807023_0001_m_000000_0' done.
2020-08-26 20:28:39,685 INFO mapred.Task: Final Counters for attempt_local1532807023_0001_m_000000_0: Counters: 21
        File System Counters
                FILE: Number of bytes read=103582307
                FILE: Number of bytes written=104919257
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=0
                HDFS: Number of bytes written=18914
                HDFS: Number of read operations=6
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=3
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=649
                Map output records=649
                Input split bytes=78
                Spilled Records=0
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=47
                Total committed heap usage (bytes)=1106771968
        File Input Format Counters
                Bytes Read=0
        File Output Format Counters
                Bytes Written=18914
2020-08-26 20:28:39,685 INFO mapred.LocalJobRunner: Finishing task: attempt_local1532807023_0001_m_000000_0
2020-08-26 20:28:39,686 INFO mapred.LocalJobRunner: map task executor complete.
2020-08-26 20:28:39,713 INFO hadoop.ParquetFileReader: Initiating action with parallelism: 5
2020-08-26 20:28:39,734 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-08-26 20:28:39,769 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-08-26 20:28:40,186 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-08-26 20:28:40,560 INFO mapreduce.Job:  map 100% reduce 0%
2020-08-26 20:28:41,562 INFO mapreduce.Job: Job job_local1532807023_0001 completed successfully
2020-08-26 20:28:41,572 INFO mapreduce.Job: Counters: 21
        File System Counters
                FILE: Number of bytes read=103582307
                FILE: Number of bytes written=104919257
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=0
                HDFS: Number of bytes written=18914
                HDFS: Number of read operations=6
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=3
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=649
                Map output records=649
                Input split bytes=78
                Spilled Records=0
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=47
                Total committed heap usage (bytes)=1106771968
        File Input Format Counters
                Bytes Read=0
        File Output Format Counters
                Bytes Written=18914
